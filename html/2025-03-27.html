<!DOCTYPE html>
<html lang="ja">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>AI & Data Science Pulse: March 26-27, 2025</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script>
      tailwind.config = {
        theme: {
          extend: {
            colors: {
              primary: "#005593",
              secondary: "#004483",
              accent: "#e6f0f9",
            },
          },
        },
      };
    </script>
    <style>
      @import url("https://fonts.googleapis.com/css2?family=Noto+Sans+JP:wght@400;500;700&display=swap");

      body {
        font-family: "Noto Sans JP", -apple-system, BlinkMacSystemFont,
          "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
      }

      .section-transition {
        transition: all 0.3s ease;
      }

      .item-hover:hover {
        transform: translateX(4px);
      }
    </style>
  </head>
  <body class="min-h-screen flex flex-col bg-slate-50">
    <!-- Header -->
    <header
      class="bg-gradient-to-r from-[#004483] to-[#005593] text-white py-10 px-4 shadow-lg"
    >
      <div class="container mx-auto max-w-5xl">
        <h1
          class="text-2xl md:text-3xl lg:text-4xl font-bold mb-2 tracking-tight"
        >
          AI & Data Science Pulse: March 26-27, 2025
        </h1>
        <p class="text-blue-100 text-sm md:text-base">
          Tracking the latest innovations and breakthroughs across the AI
          landscape
        </p>
      </div>
    </header>

    <!-- Main Content -->
    <main class="flex-grow py-8 px-4">
      <div class="container mx-auto max-w-5xl">
        <!-- Section 1: GitHub Trending -->
        <section
          class="mb-8 bg-white rounded-xl overflow-hidden shadow-md border border-slate-100 transition-all duration-300 hover:shadow-lg section-transition"
        >
          <div
            class="flex justify-between items-center p-4 bg-gradient-to-r from-[#e6f0f9] to-[#f0f7ff] cursor-pointer"
            onclick="toggleSection('github')"
          >
            <h2
              class="text-lg md:text-xl font-semibold text-[#005593] flex items-center gap-2"
            >
              <svg
                xmlns="http://www.w3.org/2000/svg"
                width="20"
                height="20"
                viewBox="0 0 24 24"
                fill="none"
                stroke="currentColor"
                stroke-width="2"
                stroke-linecap="round"
                stroke-linejoin="round"
                class="h-5 w-5"
              >
                <path
                  d="M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35 0-3.5 0 0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.403 5.403 0 0 0 4 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4"
                ></path>
                <path d="M9 18c-4.51 2-5-2-7-2"></path>
              </svg>
              <span>1. GitHub Trending</span>
            </h2>
            <svg
              xmlns="http://www.w3.org/2000/svg"
              width="20"
              height="20"
              viewBox="0 0 24 24"
              fill="none"
              stroke="currentColor"
              stroke-width="2"
              stroke-linecap="round"
              stroke-linejoin="round"
              class="h-5 w-5 text-[#005593]"
              id="github-icon"
            >
              <path d="m18 15-6-6-6 6"></path>
            </svg>
          </div>

          <div class="p-5" id="github-content">
            <ul class="space-y-4">
              <!-- khoj-ai/khoj -->
              <li
                class="border-l-4 border-[#005593] pl-4 py-2 bg-slate-50 rounded-r-lg transition-all duration-300 hover:bg-slate-100 item-hover"
              >
                <div class="flex items-start gap-2">
                  <strong>
                    <!-- Snippet includes link to trending with anchor -->
                    <a
                      href="https://github.com/trending/python#:~:text=Your%20AI%20second%20brain.%20Self,free"
                      class="text-[#005593] hover:text-[#0077cc] flex items-center"
                    >
                      khoj-ai/khoj
                      <svg
                        xmlns="http://www.w3.org/2000/svg"
                        width="14"
                        height="14"
                        viewBox="0 0 24 24"
                        fill="none"
                        stroke="currentColor"
                        stroke-width="2"
                        stroke-linecap="round"
                        stroke-linejoin="round"
                        class="h-3.5 w-3.5 ml-1"
                      >
                        <path
                          d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"
                        ></path>
                        <polyline points="15 3 21 3 21 9"></polyline>
                        <line x1="10" y1="14" x2="21" y2="3"></line>
                      </svg>
                    </a>
                  </strong>
                </div>
                <p class="mt-1 text-slate-600 text-sm">
                  オンライン/ローカルの LLM を用いて、自分専用の「AI
                  セカンドブレイン」を構築できる自己ホスト型プラットフォーム。Web
                  やドキュメントから情報を取得し、質問への回答やカスタムエージェントの作成による自動化が可能です。
                </p>
              </li>

              <!-- joanrod/star-vector -->
              <li
                class="border-l-4 border-[#005593] pl-4 py-2 bg-slate-50 rounded-r-lg transition-all duration-300 hover:bg-slate-100 item-hover"
              >
                <div class="flex items-start gap-2">
                  <strong>
                    <a
                      href="https://github.com/trending/python#:~:text=StarVector%20is%20a%20foundation%20model,SVG%20code%20with%20remarkable%20precision"
                      class="text-[#005593] hover:text-[#0077cc] flex items-center"
                    >
                      joanrod/star-vector
                      <svg
                        xmlns="http://www.w3.org/2000/svg"
                        width="14"
                        height="14"
                        viewBox="0 0 24 24"
                        fill="none"
                        stroke="currentColor"
                        stroke-width="2"
                        stroke-linecap="round"
                        stroke-linejoin="round"
                        class="h-3.5 w-3.5 ml-1"
                      >
                        <path
                          d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"
                        ></path>
                        <polyline points="15 3 21 3 21 9"></polyline>
                        <line x1="10" y1="14" x2="21" y2="3"></line>
                      </svg>
                    </a>
                  </strong>
                </div>
                <p class="mt-1 text-slate-600 text-sm">
                  SVG
                  画像を生成するためのファウンデーションモデル。視覚と言語の両モーダルを扱うアーキテクチャにより、画像とテキスト入力から高品質な
                  SVG コードを正確に生成します。
                </p>
              </li>

              <!-- deepseek-ai/DeepSeek-V3 -->
              <li
                class="border-l-4 border-[#005593] pl-4 py-2 bg-slate-50 rounded-r-lg transition-all duration-300 hover:bg-slate-100 item-hover"
              >
                <div class="flex items-start gap-2">
                  <strong>
                    <a
                      href="https://medium.com/data-science-in-your-pocket/deepseek-v3-the-best-open-source-llm-727d3421ae38#:~:text=ratio%20and%20accuracy%20,Here%E2%80%99s%20why%20it%E2%80%99s%20the%20best"
                      class="text-[#005593] hover:text-[#0077cc] flex items-center"
                    >
                      deepseek-ai/DeepSeek-V3
                      <svg
                        xmlns="http://www.w3.org/2000/svg"
                        width="14"
                        height="14"
                        viewBox="0 0 24 24"
                        fill="none"
                        stroke="currentColor"
                        stroke-width="2"
                        stroke-linecap="round"
                        stroke-linejoin="round"
                        class="h-3.5 w-3.5 ml-1"
                      >
                        <path
                          d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"
                        ></path>
                        <polyline points="15 3 21 3 21 9"></polyline>
                        <line x1="10" y1="14" x2="21" y2="3"></line>
                      </svg>
                    </a>
                  </strong>
                </div>
                <p class="mt-1 text-slate-600 text-sm">
                  総パラメータ数 6710 億（MoE 使用時は 370
                  億を活性化）を誇る超大規模言語モデル。独自の MoE
                  アーキテクチャにより効率化を図り、既存オープンソースモデルを上回る性能を達成。Claude
                  3.5 や GPT-4 に迫る精度を主張しています。
                </p>
              </li>

              <!-- browser-use/browser-use -->
              <li
                class="border-l-4 border-[#005593] pl-4 py-2 bg-slate-50 rounded-r-lg transition-all duration-300 hover:bg-slate-100 item-hover"
              >
                <div class="flex items-start gap-2">
                  <strong>
                    <a
                      href="https://github.com/trending/python#:~:text=Make%20websites%20accessible%20for%20AI,agents"
                      class="text-[#005593] hover:text-[#0077cc] flex items-center"
                    >
                      browser-use/browser-use
                      <svg
                        xmlns="http://www.w3.org/2000/svg"
                        width="14"
                        height="14"
                        viewBox="0 0 24 24"
                        fill="none"
                        stroke="currentColor"
                        stroke-width="2"
                        stroke-linecap="round"
                        stroke-linejoin="round"
                        class="h-3.5 w-3.5 ml-1"
                      >
                        <path
                          d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"
                        ></path>
                        <polyline points="15 3 21 3 21 9"></polyline>
                        <line x1="10" y1="14" x2="21" y2="3"></line>
                      </svg>
                    </a>
                  </strong>
                </div>
                <p class="mt-1 text-slate-600 text-sm">
                  AI エージェントによる Web
                  サイトの利用を実現するブラウザ自動操作フレームワーク。ブラウザ操作を抽象化し、LLM
                  が Web 上の情報を取得・操作できる環境を提供します。
                </p>
              </li>

              <!-- vllm-project/vllm -->
              <li
                class="border-l-4 border-[#005593] pl-4 py-2 bg-slate-50 rounded-r-lg transition-all duration-300 hover:bg-slate-100 item-hover"
              >
                <div class="flex items-start gap-2">
                  <strong>
                    <a
                      href="https://github.com/trending/python#:~:text=A%20high,and%20serving%20engine%20for%20LLMs"
                      class="text-[#005593] hover:text-[#0077cc] flex items-center"
                    >
                      vllm-project/vllm
                      <svg
                        xmlns="http://www.w3.org/2000/svg"
                        width="14"
                        height="14"
                        viewBox="0 0 24 24"
                        fill="none"
                        stroke="currentColor"
                        stroke-width="2"
                        stroke-linecap="round"
                        stroke-linejoin="round"
                        class="h-3.5 w-3.5 ml-1"
                      >
                        <path
                          d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"
                        ></path>
                        <polyline points="15 3 21 3 21 9"></polyline>
                        <line x1="10" y1="14" x2="21" y2="3"></line>
                      </svg>
                    </a>
                  </strong>
                </div>
                <p class="mt-1 text-slate-600 text-sm">
                  大規模言語モデル向けの高速かつ低メモリ消費の推論・サービングエンジン。独自のメモリ管理と実行最適化により、高スループットで効率的な
                  LLM 推論が可能です。
                </p>
              </li>

              <!-- agno-agi/agno -->
              <li
                class="border-l-4 border-[#005593] pl-4 py-2 bg-slate-50 rounded-r-lg transition-all duration-300 hover:bg-slate-100 item-hover"
              >
                <div class="flex items-start gap-2">
                  <strong>
                    <a
                      href="https://github.com/trending/python#:~:text=Agno%20is%20a%20lightweight%20library,memory%2C%20knowledge%2C%20tools%20and%20reasoning"
                      class="text-[#005593] hover:text-[#0077cc] flex items-center"
                    >
                      agno-agi/agno
                      <svg
                        xmlns="http://www.w3.org/2000/svg"
                        width="14"
                        height="14"
                        viewBox="0 0 24 24"
                        fill="none"
                        stroke="currentColor"
                        stroke-width="2"
                        stroke-linecap="round"
                        stroke-linejoin="round"
                        class="h-3.5 w-3.5 ml-1"
                      >
                        <path
                          d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"
                        ></path>
                        <polyline points="15 3 21 3 21 9"></polyline>
                        <line x1="10" y1="14" x2="21" y2="3"></line>
                      </svg>
                    </a>
                  </strong>
                </div>
                <p class="mt-1 text-slate-600 text-sm">
                  マルチモーダルな AI
                  エージェントを構築するための軽量ライブラリ。LLM を統一的な API
                  で扱い、エージェントに長期記憶や知識ベース、ツール操作、推論能力などを柔軟に付与できます。
                </p>
              </li>
            </ul>
          </div>
        </section>

        <!-- Section 2: Paper (arXiv) -->
        <section
          class="mb-8 bg-white rounded-xl overflow-hidden shadow-md border border-slate-100 transition-all duration-300 hover:shadow-lg section-transition"
        >
          <div
            class="flex justify-between items-center p-4 bg-gradient-to-r from-[#e6f0f9] to-[#f0f7ff] cursor-pointer"
            onclick="toggleSection('papers')"
          >
            <h2
              class="text-lg md:text-xl font-semibold text-[#005593] flex items-center gap-2"
            >
              <svg
                xmlns="http://www.w3.org/2000/svg"
                width="20"
                height="20"
                viewBox="0 0 24 24"
                fill="none"
                stroke="currentColor"
                stroke-width="2"
                stroke-linecap="round"
                stroke-linejoin="round"
                class="h-5 w-5"
              >
                <path
                  d="M14.5 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V7.5L14.5 2z"
                ></path>
                <polyline points="14 2 14 8 20 8"></polyline>
                <line x1="16" y1="13" x2="8" y2="13"></line>
                <line x1="16" y1="17" x2="8" y2="17"></line>
                <line x1="10" y1="9" x2="8" y2="9"></line>
              </svg>
              <span>2. Paper (arXiv)</span>
            </h2>
            <svg
              xmlns="http://www.w3.org/2000/svg"
              width="20"
              height="20"
              viewBox="0 0 24 24"
              fill="none"
              stroke="currentColor"
              stroke-width="2"
              stroke-linecap="round"
              stroke-linejoin="round"
              class="h-5 w-5 text-[#005593]"
              id="papers-icon"
            >
              <path d="m18 15-6-6-6 6"></path>
            </svg>
          </div>

          <div class="p-5" id="papers-content">
            <ul class="space-y-4">
              <!-- Long-Context Autoregressive Video Modeling with Next-Frame Prediction -->
              <li
                class="border-l-4 border-[#005593] pl-4 py-2 bg-slate-50 rounded-r-lg transition-all duration-300 hover:bg-slate-100 item-hover"
              >
                <div class="flex items-start gap-2">
                  <strong>
                    <a
                      href="https://arxiv.org/html/2503.19325v1#:~:text=Long,Existing%20RoPE%20lacks%20effective"
                      class="text-[#005593] hover:text-[#0077cc] flex items-center"
                    >
                      Long-Context Autoregressive Video Modeling with Next-Frame
                      Prediction
                      <svg
                        xmlns="http://www.w3.org/2000/svg"
                        width="14"
                        height="14"
                        viewBox="0 0 24 24"
                        fill="none"
                        stroke="currentColor"
                        stroke-width="2"
                        stroke-linecap="round"
                        stroke-linejoin="round"
                        class="h-3.5 w-3.5 ml-1"
                      >
                        <path
                          d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"
                        ></path>
                        <polyline points="15 3 21 3 21 9"></polyline>
                        <line x1="10" y1="14" x2="21" y2="3"></line>
                      </svg>
                    </a>
                  </strong>
                </div>
                <p class="mt-1 text-slate-600 text-sm">
                  長い時間文脈を扱うビデオ生成モデル（FAR）を提案。RoPE
                  位置エンコーディングに時間減衰を持たせた
                  <em>FlexRoPE</em>
                  を導入し、高解像度の短期コンテキストと低解像度の長期コンテキストを組み合わせることで、長尺動画生成における精度と速度を大幅に向上。既存手法を上回る性能を報告しています。
                </p>
              </li>

              <!-- Qwen2.5-Omni-7B (Alibaba Cloud) -->
              <li
                class="border-l-4 border-[#005593] pl-4 py-2 bg-slate-50 rounded-r-lg transition-all duration-300 hover:bg-slate-100 item-hover"
              >
                <div class="flex items-start gap-2">
                  <strong>
                    <a
                      href="https://www.alizila.com/alibaba-cloud-releases-qwen2-5-omni-7b-an-end-to-end-multimodal-ai-model/#:~:text=Alibaba%20Cloud%20has%20launched%C2%A0Qwen2.5,devices%20like%C2%A0mobile%20phones%20and%20laptops"
                      class="text-[#005593] hover:text-[#0077cc] flex items-center"
                    >
                      Qwen2.5-Omni-7B (Alibaba Cloud)
                      <svg
                        xmlns="http://www.w3.org/2000/svg"
                        width="14"
                        height="14"
                        viewBox="0 0 24 24"
                        fill="none"
                        stroke="currentColor"
                        stroke-width="2"
                        stroke-linecap="round"
                        stroke-linejoin="round"
                        class="h-3.5 w-3.5 ml-1"
                      >
                        <path
                          d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"
                        ></path>
                        <polyline points="15 3 21 3 21 9"></polyline>
                        <line x1="10" y1="14" x2="21" y2="3"></line>
                      </svg>
                    </a>
                  </strong>
                </div>
                <p class="mt-1 text-slate-600 text-sm">
                  アリババクラウドが発表したエッジデバイス向けマルチモーダル AI
                  モデル。70
                  億パラメータながら画像・音声・動画を統合的に処理し、
                  スマホ等のデバイスでも動作可能な軽量性と高性能を両立。音声対話や画像・動画解析など多様なタスクで最先端の性能を示しています。
                </p>
              </li>

              <!-- DeepSeek-V3 Technical Report -->
              <li
                class="border-l-4 border-[#005593] pl-4 py-2 bg-slate-50 rounded-r-lg transition-all duration-300 hover:bg-slate-100 item-hover"
              >
                <div class="flex items-start gap-2">
                  <strong>
                    <a
                      href="https://medium.com/data-science-in-your-pocket/deepseek-v3-the-best-open-source-llm-727d3421ae38#:~:text=ratio%20and%20accuracy%20,Here%E2%80%99s%20why%20it%E2%80%99s%20the%20best"
                      class="text-[#005593] hover:text-[#0077cc] flex items-center"
                    >
                      DeepSeek-V3 Technical Report
                      <svg
                        xmlns="http://www.w3.org/2000/svg"
                        width="14"
                        height="14"
                        viewBox="0 0 24 24"
                        fill="none"
                        stroke="currentColor"
                        stroke-width="2"
                        stroke-linecap="round"
                        stroke-linejoin="round"
                        class="h-3.5 w-3.5 ml-1"
                      >
                        <path
                          d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"
                        ></path>
                        <polyline points="15 3 21 3 21 9"></polyline>
                        <line x1="10" y1="14" x2="21" y2="3"></line>
                      </svg>
                    </a>
                  </strong>
                </div>
                <p class="mt-1 text-slate-600 text-sm">
                  中国発の超大規模 LLM「DeepSeek-V3」の技術報告。一度に 370
                  億パラメータのみを活性化する Mixture-of-Experts
                  構造でメモリ効率を向上。14.8 兆トークンの事前学習を行い、MMLU
                  などの知識タスクで 90 点近くを記録。Claude 3.5 や GPT-4
                  に肉薄する性能を示したと報告されています。
                </p>
              </li>

              <!-- 長文ビデオ生成における位置エンコーディングの改良 -->
              <li
                class="border-l-4 border-[#005593] pl-4 py-2 bg-slate-50 rounded-r-lg transition-all duration-300 hover:bg-slate-100 item-hover"
              >
                <div class="flex items-start gap-2">
                  <strong class="text-[#005593]">
                    長文ビデオ生成における位置エンコーディングの改良:
                  </strong>
                </div>
                <p class="mt-1 text-slate-600 text-sm">
                  上記 FAR モデルに関連する研究。既存の RoPE
                  エンコーディングが長い動画文脈への適応に問題がある点を指摘し、
                  時間減衰性を持たせた
                  <em>FlexRoPE</em>
                  を導入することで長時間シーケンスに対応。高解像度と低解像度のコンテキストを組み合わせて学習し、長尺動画生成を効率化します。
                </p>
              </li>
            </ul>
          </div>
        </section>

        <!-- Section 3: Industry Spotlight -->
        <section
          class="mb-8 bg-white rounded-xl overflow-hidden shadow-md border border-slate-100 transition-all duration-300 hover:shadow-lg section-transition"
        >
          <div
            class="flex justify-between items-center p-4 bg-gradient-to-r from-[#e6f0f9] to-[#f0f7ff] cursor-pointer"
            onclick="toggleSection('companies')"
          >
            <h2
              class="text-lg md:text-xl font-semibold text-[#005593] flex items-center gap-2"
            >
              <svg
                xmlns="http://www.w3.org/2000/svg"
                width="20"
                height="20"
                viewBox="0 0 24 24"
                fill="none"
                stroke="currentColor"
                stroke-width="2"
                stroke-linecap="round"
                stroke-linejoin="round"
                class="h-5 w-5"
              >
                <rect width="16" height="20" x="4" y="2" rx="2" ry="2"></rect>
                <path d="M9 22v-4h6v4"></path>
                <path d="M8 6h.01"></path>
                <path d="M16 6h.01"></path>
                <path d="M12 6h.01"></path>
                <path d="M12 10h.01"></path>
                <path d="M12 14h.01"></path>
                <path d="M16 10h.01"></path>
                <path d="M16 14h.01"></path>
                <path d="M8 10h.01"></path>
                <path d="M8 14h.01"></path>
              </svg>
              <span>3. Industry Spotlight</span>
            </h2>
            <svg
              xmlns="http://www.w3.org/2000/svg"
              width="20"
              height="20"
              viewBox="0 0 24 24"
              fill="none"
              stroke="currentColor"
              stroke-width="2"
              stroke-linecap="round"
              stroke-linejoin="round"
              class="h-5 w-5 text-[#005593]"
              id="companies-icon"
            >
              <path d="m18 15-6-6-6 6"></path>
            </svg>
          </div>

          <div class="p-5" id="companies-content">
            <ul class="space-y-4">
              <!-- OpenAI -->
              <li
                class="border-l-4 border-[#005593] pl-4 py-2 bg-slate-50 rounded-r-lg transition-all duration-300 hover:bg-slate-100 item-hover"
              >
                <div class="flex items-start gap-2">
                  <strong class="text-[#005593]">OpenAI:</strong>
                </div>
                <p class="mt-1 text-slate-600 text-sm">
                  セキュリティと AI 開発に関する最新ブログを公開。「AGI
                  への道のりにおけるセキュリティ」と題し、過去 2
                  年間で助成してきた
                  <strong>サイバーセキュリティ助成プログラム</strong>
                  の成果やバグバウンティ拡充など、安全性向上策について言及しました。
                  <br />
                  参考:
                  <a
                    href="https://openai.com/index/security-on-the-path-to-agi/#:~:text=Evolving%20our%20Cybersecurity%20Grant%20Program,like%20prompt%20injection%2C%20secure%20code"
                    target="_blank"
                    >Security on the path to AGI | OpenAI</a
                  >
                </p>
              </li>

              <!-- OpenAI (プロダクト) -->
              <li
                class="border-l-4 border-[#005593] pl-4 py-2 bg-slate-50 rounded-r-lg transition-all duration-300 hover:bg-slate-100 item-hover"
              >
                <div class="flex items-start gap-2">
                  <strong class="text-[#005593]">OpenAI (プロダクト):</strong>
                </div>
                <p class="mt-1 text-slate-600 text-sm">
                  CEO の Sam Altman 氏が、他社である Anthropic
                  が提唱するオープン標準「モデルコンテキストプロトコル（MCP）」を
                  OpenAI 製品群に採用すると発表。外部データやツールと AI
                  アシスタントを連携するオープンプロトコルで、ChatGPT
                  デスクトップ版や Agent SDK への対応を進めるとのこと。
                  <br />
                  参考:
                  <a
                    href="https://techcrunch.com/2025/03/26/openai-adopts-rival-anthropics-standard-for-connecting-ai-models-to-data/#:~:text=OpenAI%20is%20embracing%20rival%20Anthropic%E2%80%99s,the%20systems%20where%20data%20resides"
                    target="_blank"
                    >OpenAI adopts rival Anthropic's standard for connecting AI
                    models to data</a
                  >
                </p>
              </li>

              <!-- Google/DeepMind -->
              <li
                class="border-l-4 border-[#005593] pl-4 py-2 bg-slate-50 rounded-r-lg transition-all duration-300 hover:bg-slate-100 item-hover"
              >
                <div class="flex items-start gap-2">
                  <strong class="text-[#005593]">Google/DeepMind:</strong>
                </div>
                <p class="mt-1 text-slate-600 text-sm">
                  「Gemini 2.5」を発表。思考過程をシミュレートする “Thinking
                  Model”
                  として位置づけられ、推論能力とコード生成能力を強化。最上位の
                  <strong>Gemini 2.5 Pro Experimental</strong>
                  は複数ベンチマークで従来モデルを大きく上回り、オープン評価サイト
                  LMArena でトップに立ったと報告されています。
                  <br />
                  参考:
                  <a
                    href="https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/#:~:text=Gemini%202,strong%20reasoning%20and%20code%20capabilities"
                    target="_blank"
                    >Gemini 2.5: Our newest Gemini model</a
                  >
                </p>
              </li>

              <!-- Google (Workspace) -->
              <li
                class="border-l-4 border-[#005593] pl-4 py-2 bg-slate-50 rounded-r-lg transition-all duration-300 hover:bg-slate-100 item-hover"
              >
                <div class="flex items-start gap-2">
                  <strong class="text-[#005593]">Google (Workspace):</strong>
                </div>
                <p class="mt-1 text-slate-600 text-sm">
                  ビデオ会議サービス Google Meet における新機能を発表。AI
                  アシスタント「Gemini」活用による議事録の自動要約、アクションアイテム抽出、会議トランスクリプト連携等を追加し、会議中のリアルタイム字幕履歴閲覧を可能にするなど大幅に機能強化。
                </p>
              </li>

              <!-- Anthropic -->
              <li
                class="border-l-4 border-[#005593] pl-4 py-2 bg-slate-50 rounded-r-lg transition-all duration-300 hover:bg-slate-100 item-hover"
              >
                <div class="flex items-start gap-2">
                  <strong class="text-[#005593]">Anthropic:</strong>
                </div>
                <p class="mt-1 text-slate-600 text-sm">
                  Anthropic 提唱の「モデルコンテキストプロトコル（MCP）」が
                  OpenAI
                  にも採用されるなど、業界標準化への動きが加速。最近は対話型
                  AI「Claude」にインターネット検索機能を追加し、最新情報に基づく引用付き回答を可能にするアップデートを実施（米国の一部ユーザ向けにプレビュー提供中）。
                  <br />
                  参考:
                  <a
                    href="https://www.anthropic.com/news/web-search#:~:text=You%20can%20now%20use%20Claude,from%20the%20most%20recent%20data"
                    target="_blank"
                    >Claude can now search the web | Anthropic</a
                  >
                </p>
              </li>

              <!-- Meta -->
              <li
                class="border-l-4 border-[#005593] pl-4 py-2 bg-slate-50 rounded-r-lg transition-all duration-300 hover:bg-slate-100 item-hover"
              >
                <div class="flex items-start gap-2">
                  <strong class="text-[#005593]">Meta:</strong>
                </div>
                <p class="mt-1 text-slate-600 text-sm">
                  公式発表は少ないものの、独自 AI
                  チップのテストを開始するなどインフラ整備を加速中との報道（3 月
                  11 日付 Reuters）。オープンソースコミュニティでは Llama
                  系を中心としたモデル派生や活用が盛んに行われています。
                </p>
              </li>

              <!-- Alibaba -->
              <li
                class="border-l-4 border-[#005593] pl-4 py-2 bg-slate-50 rounded-r-lg transition-all duration-300 hover:bg-slate-100 item-hover"
              >
                <div class="flex items-start gap-2">
                  <strong class="text-[#005593]">Alibaba (アリババ):</strong>
                </div>
                <p class="mt-1 text-slate-600 text-sm">
                  上述の <strong>Qwen2.5-Omni-7B</strong> を公式ブログで発表。70
                  億規模ながらテキスト・画像・音声・動画を統合し、スマホ等でも動作可能な点をアピールしています。2
                  ヶ月前には大規模モデル <em>Qwen2.5-Max</em>
                  もリリースしており、ベンチマークで他社最新モデルに勝ると主張。
                  <br />
                  参考:
                  <a
                    href="https://www.alizila.com/alibaba-cloud-releases-qwen2-5-omni-7b-an-end-to-end-multimodal-ai-model/#:~:text=Alibaba%20Cloud%20has%20launched%C2%A0Qwen2.5,devices%20like%C2%A0mobile%20phones%20and%20laptops"
                    target="_blank"
                    >Alibaba Cloud Releases Qwen2.5-Omni-7B</a
                  >
                </p>
              </li>

              <!-- Hugging Face -->
              <li
                class="border-l-4 border-[#005593] pl-4 py-2 bg-slate-50 rounded-r-lg transition-all duration-300 hover:bg-slate-100 item-hover"
              >
                <div class="flex items-start gap-2">
                  <strong class="text-[#005593]">Hugging Face:</strong>
                </div>
                <p class="mt-1 text-slate-600 text-sm">
                  特定の新製品発表はないものの、<strong>DeepSeek-V3</strong>
                  が 3 月 25 日に公開され話題を集めています。NVIDIA
                  が同プラットフォーム上で高解像度画像生成モデルを公開するなど、依然として最新モデル共有の中心的存在です。
                </p>
              </li>
            </ul>
          </div>
        </section>

        <!-- Section 4: Blog (Zenn/Qiita/note) -->
        <section
          class="mb-8 bg-white rounded-xl overflow-hidden shadow-md border border-slate-100 transition-all duration-300 hover:shadow-lg section-transition"
        >
          <div
            class="flex justify-between items-center p-4 bg-gradient-to-r from-[#e6f0f9] to-[#f0f7ff] cursor-pointer"
            onclick="toggleSection('articles')"
          >
            <h2
              class="text-lg md:text-xl font-semibold text-[#005593] flex items-center gap-2"
            >
              <svg
                xmlns="http://www.w3.org/2000/svg"
                width="20"
                height="20"
                viewBox="0 0 24 24"
                fill="none"
                stroke="currentColor"
                stroke-width="2"
                stroke-linecap="round"
                stroke-linejoin="round"
                class="h-5 w-5"
              >
                <path d="M2 3h6a4 4 0 0 1 4 4v14a3 3 0 0 0-3-3H2z"></path>
                <path d="M22 3h-6a4 4 0 0 0-4 4v14a3 3 0 0 1 3-3h7z"></path>
              </svg>
              <span>4. Blog (Zenn/Qiita/note)</span>
            </h2>
            <svg
              xmlns="http://www.w3.org/2000/svg"
              width="20"
              height="20"
              viewBox="0 0 24 24"
              fill="none"
              stroke="currentColor"
              stroke-width="2"
              stroke-linecap="round"
              stroke-linejoin="round"
              class="h-5 w-5 text-[#005593]"
              id="articles-icon"
            >
              <path d="m18 15-6-6-6 6"></path>
            </svg>
          </div>

          <div class="p-5" id="articles-content">
            <ul class="space-y-4">
              <!-- ChatGPT『GPT-4o』画像生成機能の大幅アップデート -->
              <li
                class="border-l-4 border-[#005593] pl-4 py-2 bg-slate-50 rounded-r-lg transition-all duration-300 hover:bg-slate-100 item-hover"
              >
                <div class="flex items-start gap-2">
                  <strong>
                    <a
                      href="https://zenn.dev/nouchinho/articles/35f2a93464f0d9#:~:text=1.%20GPT,Team%2C%20%E3%81%9D%E3%81%97%E3%81%A6Free%E3%83%97%E3%83%A9%E3%83%B3%E3%81%AE%E3%83%A6%E3%83%BC%E3%82%B6%E3%83%BC%E3%81%AB%E3%81%AF%E3%80%81%E3%83%87%E3%83%95%E3%82%A9%E3%83%AB%E3%83%88%E3%81%AE%E7%94%BB%E5%83%8F%E7%94%9F%E6%88%90%E6%A9%9F%E8%83%BD%E3%81%A8%E3%81%97%E3%81%A6%E6%AE%B5%E9%9A%8E%E7%9A%84%E3%81%AB%E5%B1%95%E9%96%8B%E3%81%95%E3%82%8C%E3%81%BE%E3%81%99%E3%80%82Enterprise%20%E3%81%8A%E3%82%88%E3%81%B3%20Edu%20%E3%83%97%E3%83%A9%E3%83%B3%E3%81%AE%E3%83%A6%E3%83%BC%E3%82%B6%E3%83%BC%E3%81%B8%E3%81%AE%E6%8F%90%E4%BE%9B%E3%81%AF%E3%80%81%E8%BF%91%E6%97%A5%E4%B8%AD%E3%81%AB%E9%96%8B%E5%A7%8B%E3%81%95%E3%82%8C%E3%82%8B%E4%BA%88%E5%AE%9A%E3%81%A7%E3%81%99%E3%80%82Enterprise"
                      class="text-[#005593] hover:text-[#0077cc] flex items-center"
                    >
                      ChatGPT『GPT-4o』画像生成が大幅アップデート！
                      <svg
                        xmlns="http://www.w3.org/2000/svg"
                        width="14"
                        height="14"
                        viewBox="0 0 24 24"
                        fill="none"
                        stroke="currentColor"
                        stroke-width="2"
                        stroke-linecap="round"
                        stroke-linejoin="round"
                        class="h-3.5 w-3.5 ml-1"
                      >
                        <path
                          d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"
                        ></path>
                        <polyline points="15 3 21 3 21 9"></polyline>
                        <line x1="10" y1="14" x2="21" y2="3"></line>
                      </svg>
                    </a>
                  </strong>
                </div>
                <p class="mt-1 text-slate-600 text-sm">
                  3 月 26 日に OpenAI
                  がリリースした画像生成能力のアップデートにより、ChatGPT 上で 4
                  コマ漫画風画像やジブリ風など多彩なスタイル画像を出力可能に。ユーザー事例では赤ちゃんの成長物語を
                  4
                  コマ化するプロンプト例が紹介され、表現力の向上が話題を集めています。
                </p>
              </li>

              <!-- 開発系 AI エージェントサービスの賢い課金法 -->
              <li
                class="border-l-4 border-[#005593] pl-4 py-2 bg-slate-50 rounded-r-lg transition-all duration-300 hover:bg-slate-100 item-hover"
              >
                <div class="flex items-start gap-2">
                  <strong>
                    <a
                      href="https://zenn.dev/tkwbr999/articles/49eb9cc5217b7f#:~:text=TL%3BDR"
                      class="text-[#005593] hover:text-[#0077cc] flex items-center"
                    >
                      複数の年間課金をして分かった開発 AI
                      エージェントの課金しどころ
                      <svg
                        xmlns="http://www.w3.org/2000/svg"
                        width="14"
                        height="14"
                        viewBox="0 0 24 24"
                        fill="none"
                        stroke="currentColor"
                        stroke-width="2"
                        stroke-linecap="round"
                        stroke-linejoin="round"
                        class="h-3.5 w-3.5 ml-1"
                      >
                        <path
                          d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"
                        ></path>
                        <polyline points="15 3 21 3 21 9"></polyline>
                        <line x1="10" y1="14" x2="21" y2="3"></line>
                      </svg>
                    </a>
                  </strong>
                </div>
                <p class="mt-1 text-slate-600 text-sm">
                  コーディング支援ツールや AI
                  エージェント乱立時代の課金戦略を考察。Cursor、Claude 3.7
                  Sonnet、Windsurf、Cline、Devin、Dify
                  などに年間課金してきた著者の結論は「月額課金の Cursor
                  を使い、必要に応じて柔軟に乗り換えるのがベスト」。AI
                  ツールの進化が早いため年契約はリスクが高いと指摘。
                </p>
              </li>

              <!-- 「AIがあるんだからもっと安く早く作れるでしょ？」への回答 -->
              <li
                class="border-l-4 border-[#005593] pl-4 py-2 bg-slate-50 rounded-r-lg transition-all duration-300 hover:bg-slate-100 item-hover"
              >
                <div class="flex items-start gap-2">
                  <strong>
                    <a
                      href="https://qiita.com/ku_suke/items/577bdb839b411fe75e44#:~:text=Last%20updated%20at%202025,13"
                      class="text-[#005593] hover:text-[#0077cc] flex items-center"
                    >
                      「AI
                      があるんだからもっと安く早く作れるでしょ？」と非エンジニアに言われた時に読む記事
                      <svg
                        xmlns="http://www.w3.org/2000/svg"
                        width="14"
                        height="14"
                        viewBox="0 0 24 24"
                        fill="none"
                        stroke="currentColor"
                        stroke-width="2"
                        stroke-linecap="round"
                        stroke-linejoin="round"
                        class="h-3.5 w-3.5 ml-1"
                      >
                        <path
                          d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"
                        ></path>
                        <polyline points="15 3 21 3 21 9"></polyline>
                        <line x1="10" y1="14" x2="21" y2="3"></line>
                      </svg>
                    </a>
                  </strong>
                </div>
                <p class="mt-1 text-slate-600 text-sm">
                  Qiita に投稿された記事。AI で「30
                  秒もあればプログラムが生成できる」ように見えるが、実際の企業開発ではテストやバグ修正、人力の手間が必要で、容易にコスト削減や短納期化はできないとユーモラスに解説。800+
                  いいねを獲得し話題に。
                </p>
              </li>

              <!-- 「Tableau LangChain」を試してみた -->
              <li
                class="border-l-4 border-[#005593] pl-4 py-2 bg-slate-50 rounded-r-lg transition-all duration-300 hover:bg-slate-100 item-hover"
              >
                <div class="flex items-start gap-2">
                  <strong>
                    <a
                      href="https://zenn.dev/cavernaria/articles/ef88238ff090e4#:~:text=%E3%81%9D%E3%82%82%E3%81%9D%E3%82%82%E3%81%9AMangchain"
                      class="text-[#005593] hover:text-[#0077cc] flex items-center"
                    >
                      Tableau LangChain を試してみた
                      <svg
                        xmlns="http://www.w3.org/2000/svg"
                        width="14"
                        height="14"
                        viewBox="0 0 24 24"
                        fill="none"
                        stroke="currentColor"
                        stroke-width="2"
                        stroke-linecap="round"
                        stroke-linejoin="round"
                        class="h-3.5 w-3.5 ml-1"
                      >
                        <path
                          d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"
                        ></path>
                        <polyline points="15 3 21 3 21 9"></polyline>
                        <line x1="10" y1="14" x2="21" y2="3"></line>
                      </svg>
                    </a>
                  </strong>
                </div>
                <p class="mt-1 text-slate-600 text-sm">
                  BI ツール Tableau のデータソースに対して自然言語で質問できる
                  OSS 「tableau_langchain」を検証し、LangChain + Pinecone +
                  Tavily
                  などを組み合わせて会話的にクエリを投げる仕組みを構築。セットアップ方法や
                  Q&amp;A 実例を紹介しています。
                </p>
              </li>

              <!-- その他の話題 -->
              <li
                class="border-l-4 border-[#005593] pl-4 py-2 bg-slate-50 rounded-r-lg transition-all duration-300 hover:bg-slate-100 item-hover"
              >
                <div class="flex items-start gap-2">
                  <strong class="text-[#005593]">その他の話題:</strong>
                </div>
                <p class="mt-1 text-slate-600 text-sm">
                  Zenn では「Gemini-2.5
                  のベンチマーク未公開の理由を推測する記事」や「生成 AI
                  を用いた類似文書検索の実装と課題分析」などが公開。note
                  では有志による週間 AI ニュースまとめ、企業の生成 AI
                  活用事例レポート等が発信されています。
                </p>
              </li>
            </ul>
          </div>
        </section>
      </div>
    </main>

    <!-- Footer -->
    <footer
      class="bg-gradient-to-r from-[#004483] to-[#005593] text-white py-6 px-4"
    >
      <div class="container mx-auto max-w-5xl text-center">
        <p class="text-sm">
          © 2025 AI & Data Science Pulse | Last Updated: March 27, 2025
        </p>
      </div>
    </footer>

    <!-- JavaScript for toggling sections -->
    <script>
      // Initialize all sections as expanded
      const sectionStates = {
        github: true,
        papers: true,
        companies: true,
        articles: true,
      };

      // Function to toggle section visibility
      function toggleSection(sectionId) {
        const content = document.getElementById(sectionId + "-content");
        const icon = document.getElementById(sectionId + "-icon");

        sectionStates[sectionId] = !sectionStates[sectionId];

        if (sectionStates[sectionId]) {
          content.style.display = "block";
          icon.innerHTML = '<path d="m18 15-6-6-6 6"></path>';
        } else {
          content.style.display = "none";
          icon.innerHTML = '<path d="m6 9 6 6 6-6"></path>';
        }
      }
    </script>
  </body>
</html>
